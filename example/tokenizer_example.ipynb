{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc4e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version:  4.56.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "print('transformers version: ', transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b86eae",
   "metadata": {},
   "source": [
    "1. 编解码示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b754f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- 编码 --------\n",
      "token ids:\n",
      " tensor([[ 2517, 17459,    73,    13,    52,  3168,   346,  1557,   356,   221,\n",
      "          2301,   286,   199,  2629,  9951,    26,   199,     0,   221, 32000,\n",
      "           221, 32001,   221, 32002,   221, 32003]])\n",
      "token ids length:  26\n",
      "-------- 解码 --------\n",
      "测试 Mini-Tokenizer 效果。\n",
      "特殊字符:\n",
      "   <think> </think>\n",
      "-------- 直接使用tokenizer编码 --------\n",
      "{'input_ids': [2517, 17459, 73, 13, 52, 3168, 346, 1557, 356, 221, 2301, 286, 199, 2629, 9951, 26, 199, 0, 221, 32000, 221, 32001, 221, 32002, 221, 32003, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = \"../mini_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "text = \"测试 Mini-Tokenizer 效果。\\n特殊字符:\\n<|endoftext|> <|im_start|> <|im_end|> <think> </think>\"\n",
    "\n",
    "print(\"-------- 编码 --------\")\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "print(\"token ids:\\n\", input_ids)\n",
    "print(\"token ids length: \", len(input_ids[0]))\n",
    "\n",
    "# special token会被跳过, <think></think>作为added token，具有确定的id，但是没有被设置为special token，因此会被保留\n",
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(\"-------- 解码 --------\")\n",
    "print(decoded_text)\n",
    "\n",
    "# 直接使用tokenizer，返回input_ids、token_type_ids和attention_mask字段\n",
    "print(\"-------- 直接使用tokenizer编码 --------\")\n",
    "encoded_text = tokenizer(text, padding=\"max_length\", max_length=30)  # 会自动补4个pad token\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388d303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词为tokens:\n",
      " ['æµĭè¯ķ', 'ĠMin', 'i', '-', 'T', 'ok', 'en', 'iz', 'er', 'Ġ', 'æķĪæŀľ', 'ãĢĤ', 'Ċ', 'çī¹æ®Ĭ', 'åŃĹç¬¦', ':', 'Ċ', '<|endoftext|>', 'Ġ', '<|im_start|>', 'Ġ', '<|im_end|>', 'Ġ', '<think>', 'Ġ', '</think>']\n",
      "token ids:\n",
      " [2517, 17459, 73, 13, 52, 3168, 346, 1557, 356, 221, 2301, 286, 199, 2629, 9951, 26, 199, 0, 221, 32000, 221, 32001, 221, 32002, 221, 32003]\n",
      "还原后的tokens:\n",
      " ['æµĭè¯ķ', 'ĠMin', 'i', '-', 'T', 'ok', 'en', 'iz', 'er', 'Ġ', 'æķĪæŀľ', 'ãĢĤ', 'Ċ', 'çī¹æ®Ĭ', 'åŃĹç¬¦', ':', 'Ċ', '<|endoftext|>', 'Ġ', '<|im_start|>', 'Ġ', '<|im_end|>', 'Ġ', '<think>', 'Ġ', '</think>']\n",
      "还原后的文本:\n",
      " 测试 Mini-Tokenizer 效果。\n",
      "特殊字符:\n",
      "<|endoftext|> <|im_start|> <|im_end|> <think> </think>\n"
     ]
    }
   ],
   "source": [
    "# 分词示例\n",
    "tokens = tokenizer.tokenize(text)  # 因为是字节级BPE，所以显示结果为乱码\n",
    "print(\"分词为tokens:\\n\", tokens)\n",
    "\n",
    "# 将tokens转回id\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"token ids:\\n\", token_ids)\n",
    "\n",
    "# 将id转回tokens\n",
    "reconstructed_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"还原后的tokens:\\n\", reconstructed_tokens)\n",
    "\n",
    "# 将tokens转回文本\n",
    "reconstructed_text = tokenizer.decode(token_ids, skip_special_tokens=False)  #不跳过特殊token\n",
    "print(\"还原后的文本:\\n\", reconstructed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51973913",
   "metadata": {},
   "source": [
    "2. 特殊token示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099733a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special token:\n",
      " {'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}\n",
      "special token ids:\n",
      " [32001, 0, 32000]\n",
      "unknow token id: None\n",
      "think token id:\n",
      "<think>: [32002] \n",
      "</think>: [32003]\n",
      "vocab size: 32004\n"
     ]
    }
   ],
   "source": [
    "print(\"special token:\\n\", tokenizer.special_tokens_map)\n",
    "print(\"special token ids:\\n\", tokenizer.all_special_ids)\n",
    "print(\"unknow token id:\", tokenizer.unk_token_id)\n",
    "print(\"think token id:\\n<think>:\", tokenizer.encode(\"<think>\"), \"\\n</think>:\", tokenizer.encode(\"</think>\"))\n",
    "print(f\"vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4de2d",
   "metadata": {},
   "source": [
    "3. 对话模板示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1492244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== 对话模板测试 ========================\n",
      "1. 参数设置: tokenize=False, add_generation_prompt=False\n",
      "-------------------------------------------------------------\n",
      "<|im_start|>system\n",
      "你是一个专业的助手<|im_end|>\n",
      "<|im_start|>user\n",
      "你好<|im_end|>\n",
      "<|im_start|>assistant\n",
      "你好，我是一个专业的助手，我会尽力回答你的问题。<|im_end|>\n",
      "<|im_start|>user\n",
      "你好，请帮我分析这个问题<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "这是一个需要分析的问题，让我先思考一下...\n",
      "</think>\n",
      "\n",
      "根据我的分析，答案是...<|im_end|>\n",
      "\n",
      "-------------------------------------------------------------\n",
      "2. 参数设置: tokenize=False, add_generation_prompt=True\n",
      "-------------------------------------------------------------\n",
      "<|im_start|>user\n",
      "你好，请帮我分析这个问题<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "-------------------------------------------------------------\n",
      "3. 参数设置: tokenize=True, add_generation_prompt=True\n",
      "-------------------------------------------------------------\n",
      "[32000, 616, 356, 199, 2657, 807, 270, 1422, 2280, 537, 1010, 9502, 32001, 199, 32000, 2570, 16415, 199]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试对话-会自动格式化think结构\n",
    "print(\"======================== 对话模板测试 ========================\")\n",
    "print(\"1. 参数设置: tokenize=False, add_generation_prompt=False\")\n",
    "print('-------------------------------------------------------------')\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个专业的助手\"},\n",
    "    {\"role\": \"user\", \"content\": \"你好\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"你好，我是一个专业的助手，我会尽力回答你的问题。\"},  # 不带 think 标签\n",
    "    {\"role\": \"user\", \"content\": \"你好，请帮我分析这个问题\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"<think>这是一个需要分析的问题，让我先思考一下...</think>根据我的分析，答案是...\"}  # 带 think 标签\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "print(prompt)\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "print(\"2. 参数设置: tokenize=False, add_generation_prompt=True\")\n",
    "print('-------------------------------------------------------------')\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"你好，请帮我分析这个问题\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  # 对话补全时使用\n",
    "print(prompt)\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "print(\"3. 参数设置: tokenize=True, add_generation_prompt=True\")\n",
    "print('-------------------------------------------------------------')\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"你好，请帮我分析这个问题\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)  # 对话补全时使用\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
