{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc67ef9d",
   "metadata": {},
   "source": [
    "# 一、tokenizer示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf91aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version:  4.51.1\n",
      "tensor([[  223,  2530,  2040,  2733,   265,  2979, 11080,  2161,  3781,   821,\n",
      "          5405,   648,   266,  1040,   242,  9432,  1040,   238,  1040,   234]])\n",
      "20\n",
      " 这是经过训练的 mini tokenizer。魑魅魍魉\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "print('transformers version: ', transformers.__version__)\n",
    "\n",
    "model_name = \"./mini_tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"这是经过训练的 mini tokenizer。魑魅魍魉\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "print(len(input_ids[0]))\n",
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)  # 由于 tokenizer_config.json 中的 add_prefix_space=True，因此开头有空格\n",
    "print(decoded_text)  # 应完全还原原始文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ccfbd",
   "metadata": {},
   "source": [
    "## 1. 打印当前tokenizer的特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afa7a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特殊字符: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "特殊字符ID: [1, 2, 0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"特殊字符:\", tokenizer.special_tokens_map)\n",
    "print(\"特殊字符ID:\", tokenizer.all_special_ids)\n",
    "print(tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb098e",
   "metadata": {},
   "source": [
    "## 2. Tokenizer的其他基本用法示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5bef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果: ['Ġ', 'è¿Ļæĺ¯', 'ç»ıè¿ĩ', 'è®Ńç»ĥ', 'çļĦ', 'Ġm', 'ini', 'Ġt', 'ok', 'en', 'iz', 'er', 'ãĢĤ', 'éŃ', 'ĳ', 'éŃħ', 'éŃ', 'į', 'éŃ', 'ī']\n",
      "Token IDs: [223, 2530, 2040, 2733, 265, 2979, 11080, 2161, 3781, 821, 5405, 648, 266, 1040, 242, 9432, 1040, 238, 1040, 234]\n",
      "还原后的Tokens: ['Ġ', 'è¿Ļæĺ¯', 'ç»ıè¿ĩ', 'è®Ńç»ĥ', 'çļĦ', 'Ġm', 'ini', 'Ġt', 'ok', 'en', 'iz', 'er', 'ãĢĤ', 'éŃ', 'ĳ', 'éŃħ', 'éŃ', 'į', 'éŃ', 'ī']\n",
      "还原后的文本:  这是经过训练的 mini tokenizer。魑魅魍魉\n"
     ]
    }
   ],
   "source": [
    "# 分词示例\n",
    "tokens = tokenizer.tokenize(text)  # 字节级BPE，显示结果可能为乱码\n",
    "print(\"分词结果:\", tokens)\n",
    "\n",
    "# 将tokens转回ID\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# 将ID转回tokens\n",
    "reconstructed_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"还原后的Tokens:\", reconstructed_tokens)\n",
    "\n",
    "# 将ID转回文本\n",
    "reconstructed_text = tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "print(\"还原后的文本:\", reconstructed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b848c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 32000\n",
      "<s>system\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！</s>\n",
      "<s>user\n",
      "你好</s>\n",
      "<s>assistant\n",
      "\n",
      "<s>system\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！</s>\n",
      "<s>user\n",
      "你好</s>\n",
      "<s>assistant\n",
      "你好，我是你的助手！</s>\n",
      "\n",
      "[1, 2888, 13125, 201, 1986, 2057, 22430, 10825, 540, 8309, 262, 8808, 1053, 1143, 17050, 7606, 2584, 2, 30362, 1, 223, 1290, 648, 201, 1986, 1020, 2, 30362, 1, 223, 4536, 2895, 3245, 201, 1986, 1020, 262, 15565, 11852, 10001, 2584, 2, 30362, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "<s>\n",
      " </s>\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# 测试一段对话\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"<s>system\\n你是一个优秀的聊天机器人，总是给我正确的回应！</s>\\n\"},\n",
    "    {\"role\": \"user\", \"content\": '你好'},\n",
    "    {\"role\": \"assistant\", \"content\": '你好，我是你的助手！'}\n",
    "]\n",
    "\n",
    "prompt_messages = messages[:-1] # 只包含 System 和 User\n",
    "to_be_masked_prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(to_be_masked_prompt)\n",
    "\n",
    "\n",
    "# 使用模板进行文本处理\n",
    "new_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(new_prompt)  # 模型将会从<s>assistant后开始生成，到</s>结束\n",
    "new_prompt = tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "new_prompt = new_prompt + [0] * 3\n",
    "print(new_prompt)\n",
    "prompt_len = len(to_be_masked_prompt)\n",
    "new_prompt[:prompt_len] = [0] * prompt_len\n",
    "new_prompt[-3:] = [0] * 3\n",
    "print(new_prompt)\n",
    "print(tokenizer.decode(new_prompt, skip_special_tokens=False))\n",
    "\n",
    "print(tokenizer.decode([1, 201, 223, 2], skip_special_tokens=False))  # 201 是换行，223 是空格"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108783a",
   "metadata": {},
   "source": [
    "# 二、数据构造示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26dd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "seq_monkey_file_path = 'F:/BaiduNetdiskDownload/data/pretrain_data/mobvoi_seq_monkey_general_open_corpus.jsonl'\n",
    "\n",
    "# 加载训练好的分词器路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mini_tokenizer\")\n",
    "bos_token = tokenizer.bos_token\n",
    "eos_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17cd25",
   "metadata": {},
   "source": [
    "## 1. 检查数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234c65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取文件总行数...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking JSONL format: 100%|██████████| 13000000/13000000 [01:54<00:00, 113800.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查完成，文件中共有 13000000 行有效的 JSON 数据，0 行无效的 JSON 数据。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_total_lines(file_path):\n",
    "    \"\"\"\n",
    "    获取 JSONL 文件的总行数，不忽略错误，保证能够全面统计。\n",
    "    \"\"\"\n",
    "    print('获取文件总行数...')\n",
    "    with open(file_path, 'rb') as f:  # 使用二进制模式避免编码问题\n",
    "        return sum(1 for _ in f)\n",
    "    \n",
    "def check_jsonl_format(file_path):\n",
    "    \"\"\"\n",
    "    检查 JSONL 文件中的每一行是否是有效的 JSON 格式，带进度显示，并统计所有有问题的行。\n",
    "    \"\"\"\n",
    "    total_lines = get_total_lines(file_path)  # 获取文件总行数\n",
    "    valid_lines = 0\n",
    "    invalid_lines = 0\n",
    "\n",
    "    # 使用逐行读取，捕获 JSON 和编码错误\n",
    "    with open(file_path, 'rb') as f:  # 使用二进制读取避免编码问题\n",
    "        # 使用 tqdm 进度条显示检查进度\n",
    "        for idx, line in tqdm(enumerate(f), total=total_lines, desc=\"Checking JSONL format\"):\n",
    "            try:\n",
    "                # 先尝试将每行数据解码为 UTF-8\n",
    "                decoded_line = line.decode('utf-8')\n",
    "                # 然后检查是否是有效的 JSON 格式\n",
    "                obj = jsonlines.Reader([decoded_line]).read()\n",
    "                valid_lines += 1\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"Encoding error at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "            except jsonlines.InvalidLineError as e:\n",
    "                print(f\"Invalid JSON at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "\n",
    "    print(f\"检查完成，文件中共有 {valid_lines} 行有效的 JSON 数据，{invalid_lines} 行无效的 JSON 数据。\")\n",
    "    return valid_lines, invalid_lines\n",
    "\n",
    "def remove_invalid_line(file_path, output_path, invalid_line_num):\n",
    "    \"\"\"\n",
    "    读取文件，跳过指定的无效行，并将结果写入新文件\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as infile, open(output_path, 'wb') as outfile:\n",
    "        for idx, line in enumerate(infile):\n",
    "            if idx + 1 != invalid_line_num:  # 跳过无效行\n",
    "                outfile.write(line)\n",
    "\n",
    "valid_lines, invalid_lines = check_jsonl_format(seq_monkey_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a8035",
   "metadata": {},
   "source": [
    "## 2. 如何构造预训练数据\n",
    "\n",
    "每个预训练数据需要构造为如下形式：`<bos> text <eos>`，然后进行拼接：`<bos> text1 <eos> <bos> text2 <eos>`\n",
    "\n",
    "【思考】：可不可以不加`<bos>`，只用`<eos>`分割不同样本？\n",
    "\n",
    "建议同时加上`<bos>`和`<eos>`，可能的原因是：`<bos>`标记序列的起始，`<eos>`标记序列的结束；如果加入`<bos>`，一方面，在预训练时，**模型就已经见过这个特殊token的起始含义，在后续的后训练中，能够很好的适应生成任务**；另一方面，拼接后，`<eos>`之后固定为`<bos>`，即模型能够学习到`<eos><bos>`强烈的结构信号，这样即便是产生跨样本注意，当学习到`<eos><bos>`抽象的边界含义时，**会使模型自适应的将前一个样本的注意力权重减小，而只关注本样本的token，因此通常也可不使用样本掩码，避免计算开销**。\n",
    "\n",
    "---\n",
    "\n",
    "【例子】：假设有三个样本序列：`abc`,`defgh`,`ijklmn`，那么加入`<bos>`和`<eos>`并拼接，有`＜bos＞abc＜eos＞＜bos＞defgh＜eos＞＜bos＞ijklmn＜eos＞`\n",
    "\n",
    "假设`max_seq_len=3`，那么对拼接数据直接分块以构造数据集，每个块的长度为`max_seq_len+1`，以方便截取`input`和`target`，则原始数据分块后为：\n",
    "\n",
    "- 块 1: `<bos> a b c`\n",
    "- 块 2: `<eos> <bos> d e`\n",
    "- 块 3: `f g h <eos>`\n",
    "- 块 4: `<bos> i j k`\n",
    "- 块 5: `l m n <eos>`\n",
    "\n",
    "模型的`input`为：\n",
    "\n",
    "- 块 1: `<bos> a b`\n",
    "- 块 2: `<eos> <bos> d`\n",
    "- 块 3: `f g h`\n",
    "- 块 4: `<bos> i j`\n",
    "- 块 5: `l m n`\n",
    "\n",
    "预测下一个token，即`target`为：\n",
    "\n",
    "- 块 1: `a b c`\n",
    "- 块 2: `<bos> d e`\n",
    "- 块 3: `g h <eos>`\n",
    "- 块 4: `i j k`\n",
    "- 块 5: `m n <eos>`\n",
    "\n",
    "但上述直接分块会使得样本的上下文信息缺失。例如样本`defgh`，在块2中，模型只能通过`d`预测`e`，但`e`后该预测什么，模型没有见过；在块3中，模型从`f`预测`g`，但前文信息`de`模型也没见过。因此，可以采用滑动窗口的策略在一定程度上减少上下文信息缺失。例如，如果滑动窗口步长为2，那么构造的原始数据块为：\n",
    "\n",
    "- 块 1: `<bos> a b c`\n",
    "- 块 2: `b c <eos> <bos>`\n",
    "- 块 3: `<eos> <bos> d e`\n",
    "- 块 4: `d e f g`\n",
    "- 块 5: `f g h <eos>`\n",
    "- 块 6: `<bos> i j k`\n",
    "- 块 7: `j k l m`\n",
    "- 块 8: `l m n <eos>`\n",
    "\n",
    "这会使得数据量增加，但能保证信息的连贯性。\n",
    "\n",
    "结合上述示例，我们将使用滑动窗口的方式来构造数据集，窗口选择为最大长度的10%。\n",
    "\n",
    "### 1. 处理seq monkey数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc928f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇量大小: 32000\n",
      "开始处理文件: F:/BaiduNetdiskDownload/data/pretrain_data/mobvoi_seq_monkey_general_open_corpus.jsonl\n",
      "获取文件总行数...\n",
      "文件总行数: 13000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 13000000/13000000 [3:11:48<00:00, 1129.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入最后 198601 个 tokens。\n",
      "------------------------------\n",
      "处理完成!\n",
      "总共写入 7812198601 个 Token ID 到 F:/BaiduNetdiskDownload/data/pretrain_data/pretrain_data.bin\n",
      "使用的数据类型: uint16\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_seq_monkey(jsonl_path: str, bin_path: str, buffer_size: int = 1000000, dtype_str: str = 'uint16'):\n",
    "    \"\"\"\n",
    "    读取 jsonl 文件, 提取文本字段, 分词, 并将 token id 保存为二进制文件\n",
    "\n",
    "    Args:\n",
    "        jsonl_path (str): 输入的 jsonl 文件路径。\n",
    "        bin_path (str): 输出的二进制文件路径。\n",
    "        buffer_size (int): 写入磁盘前在内存中缓冲的 token id 数量。\n",
    "        dtype_str (str): 保存 token id 的 numpy 数据类型 ('uint16', 'uint32'等), 'uint16' 适用于词汇量 < 65536 的分词器, 如果词汇量更大，请使用 'uint32'。\n",
    "    \"\"\"\n",
    "    # 选择合适的 NumPy 数据类型\n",
    "    try:\n",
    "        dtype = np.dtype(dtype_str)\n",
    "    except TypeError:\n",
    "        print(f\"错误：无效的 dtype_str '{dtype_str}'。请使用 'uint16', 'uint32' 等。\")\n",
    "        return\n",
    "\n",
    "    print(f\"词汇量大小: {len(tokenizer)}\")\n",
    "    if dtype == np.uint16 and len(tokenizer) > 65535:\n",
    "        print(f\"警告：分词器词汇量大小 ({len(tokenizer)}) 可能超过了 'uint16' 的最大值 (65535)。考虑使用 'uint32'。\")\n",
    "    \n",
    "    token_buffer = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    print(f\"开始处理文件: {jsonl_path}\")\n",
    "    try:\n",
    "        # 获取文件总行数用于进度条\n",
    "        print(\"获取文件总行数...\")\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f_in:\n",
    "            total_lines = sum(1 for _ in f_in)\n",
    "            print(f\"文件总行数: {total_lines}\")\n",
    "\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f_in, open(bin_path, 'wb') as f_out:\n",
    "\n",
    "            # 使用 tqdm 显示进度\n",
    "            for line in tqdm(f_in, total=total_lines, desc=\"Processing lines\"):\n",
    "                try:\n",
    "                    # 解析 jsonl 行\n",
    "                    data = json.loads(line.strip())\n",
    "                    \n",
    "                    # 提取文本\n",
    "                    text = data.get('text')\n",
    "                    # 分词\n",
    "                    token_ids = tokenizer.encode(bos_token+text+eos_token)\n",
    "                    # 添加到缓冲区\n",
    "                    token_buffer.extend(token_ids)\n",
    "                    \n",
    "                    # 如果缓冲区达到大小，则写入文件\n",
    "                    if len(token_buffer) >= buffer_size:\n",
    "                        array_to_write = np.array(token_buffer[:buffer_size], dtype=dtype)\n",
    "                        array_to_write.tofile(f_out)\n",
    "                        total_tokens += len(array_to_write)\n",
    "                        token_buffer = token_buffer[buffer_size:] # 保留剩余部分\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"警告：无法解析 jsonl 行: {line.strip()}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"处理行时发生意外错误: {e} - 行内容: {line.strip()}\")\n",
    "\n",
    "            # 处理结束后，写入缓冲区中剩余的 token\n",
    "            if token_buffer:\n",
    "                array_to_write = np.array(token_buffer, dtype=dtype)\n",
    "                array_to_write.tofile(f_out)\n",
    "                total_tokens += len(array_to_write)\n",
    "                print(f\"写入最后 {len(token_buffer)} 个 tokens。\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：输入文件未找到 {jsonl_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时发生错误: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"处理完成!\")\n",
    "    print(f\"总共写入 {total_tokens} 个 Token ID 到 {bin_path}\")\n",
    "    print(f\"使用的数据类型: {dtype.name}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "process_seq_monkey(seq_monkey_file_path, 'F:/BaiduNetdiskDownload/data/pretrain_data/pretrain_data.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "320da573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unk_token_chunked(bin_path, dtype=np.uint16, unk_token_id=0, chunk_size_items=1024*1024*50):\n",
    "    \"\"\"\n",
    "    分块统计二进制文件中的 UNK token 的数量，并显示进度。\n",
    "\n",
    "    Args:\n",
    "        bin_path (str): 二进制文件的路径。\n",
    "        dtype (np.dtype): 文件中数据项的 numpy 数据类型。\n",
    "        unk_token_id (int): 要计数的 UNK token 的 ID。\n",
    "        chunk_size_items (int): 每次读取的数据项数量（不是字节数）。\n",
    "                                默认值约为 100MB (50 * 1024 * 1024 * 2 bytes)。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_size_bytes = os.path.getsize(bin_path)\n",
    "        item_size = np.dtype(dtype).itemsize\n",
    "        if item_size == 0:\n",
    "             print(f\"警告：无法确定数据类型 '{dtype}' 的大小。\")\n",
    "             return\n",
    "        total_items = total_size_bytes // item_size\n",
    "        print(f\"文件路径: {bin_path}\")\n",
    "        print(f\"总数据项数: {total_items:,}\")\n",
    "        print(f\"数据类型: {dtype}\")\n",
    "        print(f\"查找的 Token ID: {unk_token_id}\")\n",
    "        print(f\"分块大小（项）: {chunk_size_items}\")\n",
    "\n",
    "        total_unk_count = 0\n",
    "        processed_items = 0\n",
    "\n",
    "        with open(bin_path, 'rb') as f, tqdm(total=total_items, unit='item', desc=\"处理进度\") as pbar:\n",
    "            while True:\n",
    "                # 从文件读取一个数据块\n",
    "                data_chunk = np.fromfile(f, dtype=dtype, count=chunk_size_items)\n",
    "\n",
    "                # 如果没有读取到数据，说明文件已读完\n",
    "                if data_chunk.size == 0:\n",
    "                    break\n",
    "\n",
    "                # 统计当前块中的 UNK token\n",
    "                chunk_unk_count = np.sum(data_chunk == unk_token_id)\n",
    "                total_unk_count += chunk_unk_count\n",
    "\n",
    "                # 更新进度条\n",
    "                pbar.update(data_chunk.size)\n",
    "                processed_items += data_chunk.size\n",
    "\n",
    "        print(f\"\\n处理完成。\")\n",
    "        print(f\"总共找到的 UNK token 数量: {total_unk_count}\")\n",
    "        print(f\"总共处理的数据项: {processed_items}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件未找到 {bin_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中发生错误: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6034ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件路径: F:/BaiduNetdiskDownload/data/pretrain_data/pretrain_data.bin\n",
      "总数据项数: 7,812,198,601\n",
      "数据类型: <class 'numpy.uint16'>\n",
      "查找的 Token ID: 0\n",
      "分块大小（项）: 52428800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理进度: 100%|██████████| 7812198601/7812198601 [00:27<00:00, 281220490.12item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理完成。\n",
      "总共找到的 UNK token 数量: 0\n",
      "总共处理的数据项: 7812198601\n",
      "<s> 在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。在计算税款损失时，要不要将进项留抵税额包括在内？\n",
      "对此，实务中存在意见分歧。\n",
      "有人主张归并，即计算税款损失时包括进项留抵税额；\n",
      "有人主张剥离，即计算税款损失时剔除进项留抵税额。分析这个问题，需要确定进项留抵税额与税款损失之间是什么关系。\n",
      "理清这二者之间的关系，首先需要了解增值税的概念和其抵扣机制。增值税是以商品（货物、服务等）在流转过程中产生的增值额作为计税依据而征收的一种流转税。为避免重复征税，在增值税中存在抵扣链条机制。\n",
      "一般而言，交易上游企业缴纳的税额，交易下游\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_unk_token_chunked('F:/BaiduNetdiskDownload/data/pretrain_data/pretrain_data.bin')\n",
    "\n",
    "# 查看pretrain_data.bin的数据\n",
    "token_ids_example = np.fromfile('F:/BaiduNetdiskDownload/data/pretrain_data/pretrain_data.bin', dtype=np.uint16, count=200)\n",
    "print(tokenizer.decode(token_ids_example.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315ab40",
   "metadata": {},
   "source": [
    "### 2. 处理wikipedia数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9791b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇量大小: 32000\n",
      "开始处理文件: F:/BaiduNetdiskDownload/data/pretrain_data/wikipedia-cn-20230720-filtered.json\n",
      "获取文件总行数...\n",
      "文件总行数: 254547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 254547/254547 [03:22<00:00, 1258.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入最后 185891 个 tokens。\n",
      "------------------------------\n",
      "处理完成!\n",
      "总共写入 121185891 个 Token ID 到 F:/BaiduNetdiskDownload/data/pretrain_data/wikipedia.bin\n",
      "使用的数据类型: uint16\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def process_wikipedia(json_path: str, bin_path: str, buffer_size: int = 1000000, dtype_str: str = 'uint16'):\n",
    "    \"\"\"\n",
    "    读取 json 文件, 提取文本字段, 分词, 并将 token id 保存为二进制文件\n",
    "\n",
    "    Args:\n",
    "        json_path (str): 输入的 jsonl 文件路径。\n",
    "        bin_path (str): 输出的二进制文件路径。\n",
    "        buffer_size (int): 写入磁盘前在内存中缓冲的 token id 数量。\n",
    "        dtype_str (str): 保存 token id 的 numpy 数据类型 ('uint16', 'uint32'等), 'uint16' 适用于词汇量 < 65536 的分词器, 如果词汇量更大，请使用 'uint32'。\n",
    "    \"\"\"\n",
    "    # 选择合适的 NumPy 数据类型\n",
    "    try:\n",
    "        dtype = np.dtype(dtype_str)\n",
    "    except TypeError:\n",
    "        print(f\"错误：无效的 dtype_str '{dtype_str}'。请使用 'uint16', 'uint32' 等。\")\n",
    "        return\n",
    "\n",
    "    print(f\"词汇量大小: {len(tokenizer)}\")\n",
    "    if dtype == np.uint16 and len(tokenizer) > 65535:\n",
    "        print(f\"警告：分词器词汇量大小 ({len(tokenizer)}) 可能超过了 'uint16' 的最大值 (65535)。考虑使用 'uint32'。\")\n",
    "    \n",
    "    token_buffer = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    print(f\"开始处理文件: {json_path}\")\n",
    "    # 获取文件总行数用于进度条\n",
    "    print(\"获取文件总行数...\")\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        total_lines = len(data)\n",
    "        print(f\"文件总行数: {total_lines}\")\n",
    "\n",
    "    with open(json_path, 'r', encoding='utf-8') as f_in, open(bin_path, 'wb') as f_out:\n",
    "        total_data = json.load(f_in)\n",
    "        # 使用 tqdm 显示进度\n",
    "        for data in tqdm(total_data, total=total_lines, desc=\"Processing lines\"):\n",
    "            # 提取文本\n",
    "            text = data.get('completion')\n",
    "            # 分词\n",
    "            token_ids = tokenizer.encode(bos_token+text+eos_token)\n",
    "            # 添加到缓冲区\n",
    "            token_buffer.extend(token_ids)\n",
    "            \n",
    "            # 如果缓冲区达到大小，则写入文件\n",
    "            if len(token_buffer) >= buffer_size:\n",
    "                array_to_write = np.array(token_buffer[:buffer_size], dtype=dtype)\n",
    "                array_to_write.tofile(f_out)\n",
    "                total_tokens += len(array_to_write)\n",
    "                token_buffer = token_buffer[buffer_size:] # 保留剩余部分\n",
    "\n",
    "        # 处理结束后，写入缓冲区中剩余的 token\n",
    "        if token_buffer:\n",
    "            array_to_write = np.array(token_buffer, dtype=dtype)\n",
    "            array_to_write.tofile(f_out)\n",
    "            total_tokens += len(array_to_write)\n",
    "            print(f\"写入最后 {len(token_buffer)} 个 tokens。\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"处理完成!\")\n",
    "    print(f\"总共写入 {total_tokens} 个 Token ID 到 {bin_path}\")\n",
    "    print(f\"使用的数据类型: {dtype.name}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "wikipedia_cn_file_path = 'F:/BaiduNetdiskDownload/data/pretrain_data/wikipedia-cn-20230720-filtered.json'\n",
    "process_wikipedia(wikipedia_cn_file_path, 'F:/BaiduNetdiskDownload/data/pretrain_data/wikipedia.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d24b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件路径: F:/BaiduNetdiskDownload/data/pretrain_data/wikipedia.bin\n",
      "总数据项数: 121,185,891\n",
      "数据类型: <class 'numpy.uint16'>\n",
      "查找的 Token ID: 0\n",
      "分块大小（项）: 52428800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理进度: 100%|██████████| 121185891/121185891 [00:00<00:00, 352551043.78item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理完成。\n",
      "总共找到的 UNK token 数量: 0\n",
      "总共处理的数据项: 121185891\n",
      "<s> 昭通机场（ZPZT）是位于中国云南昭通的民用机场，始建于1935年，1960年3月开通往返航班“昆明－昭通”，原来属军民合用机场。1986年机场停止使用。1991年11月扩建，于1994年2月恢复通航。是西南地区「文明机场」，通航城市昆明。 机场占地1957亩，飞行区等级为4C，有一条跑道，长2720米，宽48米，可供波音737及以下机型起降。机坪面积6600平方米，停机位2个，航站楼面积1900平方米。位于城东6公里处，民航路与金鹰大道交叉处。\n",
      "航点\n",
      "客服电话\n",
      "昭通机场客服电话：0870-2830004</s><s> 我的英雄学院：英雄新世纪\n",
      "《我的英雄学院剧场版：英雄新世纪》（仆のヒーローアカデミア THE MOVIE ヒーローズ:ライジング\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count_unk_token_chunked('F:/BaiduNetdiskDownload/data/pretrain_data/wikipedia.bin')\n",
    "\n",
    "token_ids_example = np.fromfile('F:/BaiduNetdiskDownload/data/pretrain_data/wikipedia.bin', dtype=np.uint16, count=200)\n",
    "print(tokenizer.decode(token_ids_example.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34814f93",
   "metadata": {},
   "source": [
    "### 3. 百度百科数据示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e47bdb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '红色食品', 'summary': '红色食品是指食品为红色、橙红色或棕红色的食品。科学家认为，多吃些红色食品可预防感冒。红色食品有红柿椒、西红柿、胡萝卜、红心白薯、红果（山楂）、红苹果、草莓、红枣、老南瓜、红米、柿子等。 有治疗缺铁性贫血和缓解疲劳的作用，对乳腺癌等肿瘤疾病有防治作用，给人以兴奋感，有增加食欲，光洁皮肤，增强表皮细胞再生和防止皮肤衰老，预防感冒等作用。', 'sections': [{'title': '简介', 'content': '红色食品富含番茄红素、胡萝卜素、铁和部分氨基酸，是优质蛋白质、碳水化合物、膳食纤维、B族维生素和多种无机盐的重要来源，可以弥补粳米、白面中的营养缺失。经常食用红色食品，可以进一步提高对主食中营养的利用率，山植等食品还有治疗癌症的功效。被称为“红色生力军。”营养学家认为，红色蔬果最典型的优势在于它们都是富含天然铁质的食物，例如我们常吃的樱桃、大枣等都是贫血患者的天然良药，也适合女性经期失血后的滋补。所以，红色蔬果，女人尽可放心多吃。红色食品中还含有致病微生物的“杀手”——巨噬细胞，可以有效地抵御感冒病毒等微生物，增强人体抵抗感冒的能力。\\xa0\\xa0红色食品\\n在所有的果蔬当中，名声最好的莫过于苹果。西方有“One apple a day，keeps the doctors away．”的说法，因为苹果性情温和，含有各种维生素和微量元素，是所有的水果中最接近完美的一个。\\n还有一种说法：红色食品是相对于绿色食品而言的，指对人体有害的食品，如各种有毒有害、腐败变质、添加非食用物质的食品。红色食品危害人体健康乃至生命安全，对人体健康亮起了红灯，应当大力查处。'}, {'title': '作用', 'content': '这些食品中富含β-胡萝卜素和维生素A，对孩子上皮组织和呼吸道粘膜有很强的保护作用，可提高预防感冒的能力。\\n假如你生来体质较弱，易受感冒病毒的困扰，或者已经被感冒缠上了，红色食品会助你一臂之力，天生具有促进人体健康卫士之一的巨噬细胞活力的功能，巨噬细胞乃是感冒病毒等致病微生物的“杀手”，其活力增强了，感冒病毒自然难以在人体内立足，更谈不上生长繁殖了。至于颜色较辣椒稍浅一些的胡萝卜，所含的胡萝卜素可在体内转化为维生素A，发挥护卫人体上皮组织如呼吸道黏膜的作用，常食之同样可以增强人体抗御感冒的能力。除了红辣椒、胡萝卜外，苋菜、洋葱、红枣、番茄、红薯、山楂、苹果、草莓、老南瓜、红米等亦具此功。'}, {'title': '红色食品与感冒', 'content': '冬令时节，气候寒冷，万物收藏，人的机体生理功能处于降低、抑制、收缩状态，易患感冒，吃红色食品可扶正祛邪，增强免疫力，预防得病。[1]\\n蔬菜中的红萝卜、红辣椒、番茄、红薯、红枣、红苋菜等红色食品中，富含β-胡萝卜素，不但能清除人体氧自由基，而且参与合成维生素A，对人体上皮组织和 呼吸道黏膜有很强的保护作用。推荐以下几个食疗方———\\n★番茄猪肝汤\\n用料：猪肝250克，虾仁25克，蘑菇40克，鸡蛋1只，番茄150克，黄酒、葱段、姜片、胡椒粉、精盐适量。\\n制法：将猪肝切去筋膜洗净，切丁后加上酒、姜汁、蛋液、盐、胡椒粉，搅打成浆。用旺火蒸10—15分钟至结膏。清水加虾仁、黄酒沸煮5分钟后倒入蘑菇、番茄丁和肝膏，再煮沸，调味即可。\\n功用：养肝明目，增强免疫力。用于防感冒、防治夜盲症及免疫力低下者，以及甲亢。\\n方解：猪肝有养肝明目、增强免疫力；蘑菇补益脾胃，益阴养肝，降压，降脂，润燥化痰，增加白细胞；虾仁、番茄均有增强免疫力的食品，番茄可增强营养，减少感冒的发生。\\n★红萝卜炖牛肉\\n用料：牛肉250克、红萝卜250克。\\n制法：牛肉切成小块，加黄酒、姜、葱等配料，再加入红萝卜块，炖熟，即可食用。\\n功用：益气养胃、强健筋骨、增强免疫力。适用于防感冒及免疫力低、虚损消瘦、腰膝酸软者。\\n方解：牛肉补脾胃、益气血、强筋骨，红萝卜增强免疫力，防感冒，健脾，补血，助消化。\\n★蜂蜜红萝卜汁\\n用红萝卜汁与蜂蜜各半制成混合汁剂，每天饮用3次，每次1汤匙。可防治伤风、感冒和咽喉炎。胡萝卜能提供丰富的维生素A，具有促进机体正常生长与繁殖、维护上皮组织、防止感冒及保持视力正常。蜂蜜能补中，润燥，止痛，解毒，清热。'}, {'title': '红色食品与红肉', 'content': '红色食品是指外表呈红色的果蔬和“红肉”类。红色果蔬包括红辣椒、西红柿、红枣、山楂、草莓、苹果等，红色果蔬含有糖和多种维生素，尤其富含维生素C。“红肉”指牛肉、猪肉、羊肉及其制品。\\n红色果蔬中的辣椒具有温中散寒，开胃除湿之功效，辣椒中的辣椒素能刺激唾液和胃肠道消化液的分泌，还能刺激心血管系统，使心跳加快，血液循环加速，因此在寒冷环境有祛风除湿的作用。风寒型感冒病人食用辣椒汤能帮助发汗，有利于感冒的康复，但是胃肠疾病、结核病人则不适合食用。西红柿-在国外享有“金苹果”之称，具有较高的价值。由于西红柿含有94%左右的水分，生吃能防治中暑，止渴生津，凉血解毒的作用，但食西红柿时尽量少放盐，为了避免维生素的破坏，做汤时最好等水开了再下西红柿，而且忌食未成熟的西红柿胃，胃肠虚寒者即慢性腹泻和消化不良者应忌食之。红枣在国外被称为“天然维生素丸”，具有很好的补血功效，能安神和补益脾胃，但胃肠积滞和患有牙齿疾病者应忌食。食用红枣时不宜与鱼同食，同食易引起腹部胀痛。\\n红色食品中的肉类即所谓“红肉”，主要含蛋白质和脂肪及其它无机盐等，因此具有丰富的营养价值。不过“红肉”致癌，世界癌症研究基金会建议食用“红肉”时，每日每人撮入量应少于80克，这是因为“红肉”在烧烤、烙制、煎炸时，其表面产生多种杂环胺——致癌物。'}, {'title': '好处', 'content': '红色不但能让人联想到爱情和激情，还是一种与心脏、大脑和泌尿系统的健康有关的颜色。红色的水果和蔬菜对我们的身体健康大有裨益。 红色的果蔬主要含有丰富的植物化学成分，包括抗细胞因子、抗氧化剂和番茄红素(一种产生红色的色素)。这些成分能够预防癌症，特别是肺癌、前列腺癌和消化道癌。它们还可以延缓衰老，并且有利于防止黄斑变性。黄斑变性是导致65岁以上老年人失明的主要诱因。\\n1、草莓含有80%的水分、丰富的维生素C、少量膳食纤维和钾。由于含糖量很低，因此经常出现在减肥食谱中。大量抗坏血酸、凝集素和果胶使它成为降低血液中胆固醇含量的理想食物。传统医学中草莓可以作为润肤剂、净化剂和利胆剂，还具有镇咳和抗风湿的作用。人们还认为它有抗贫血和滋补的功效，可以促进机体生长。草莓叶子的浸剂还可以用于肠道消炎。\\n2、每天生食6至1 2颗樱桃对痛风和尿酸过多有显著疗效。樱桃的果汁有利于治疗腹泻和结肠炎，所含的抗氧化剂具有保健作用。樱桃含有的主要养分和膳食纤维较少，但是维生素B的含量不低。在矿物质中，钾和铁的含量较高。\\n3、西瓜是水分含量最高的水果，高达其总重量的95%，因此可以促进肾脏更好地发挥功能，将废物和有毒物质排出体外。\\n4、覆盆子含有丰富的维生素E、多种植物营养素和不可溶性纤维。除了具有利尿和通便作用，它还可以用于治疗风湿。\\n5、红苹果富含果胶、糖分和维生素C。此外，由于它是温和的通便剂，所以还具有特殊的医疗效用，可以用于治疗肠道功能紊乱。因此，自然医学认为红苹果可以抗腹泻、贫血和哮喘。它还能够缓解神经系统紧张，促进睡眠。每天晚上吃一个红苹果有助于迅速入睡。它还对希望保持体形的人有用，因为几乎不含脂肪，每100克只有不到58卡路里的热量。据法国图卢兹大学的研究结果显示，每天吃一个大苹果可以在8个星期内使胆固醇水平降低。\\n6、红辣椒中抗氧化剂、硒和维生素C的含量很高，甚至高于柑桔和柠檬等酸味水果。红辣椒所含的膳食纤维能够控制血液中的胆固醇和葡萄糖，还可以改善肠道功能。\\n7、红萝卜有益于治疗呼吸系统疾病，例如咽炎和喉炎，还可以减轻喉咙嘶哑。在柠檬的辅助下，它还可以用来防治哮喘和鼻窦炎。萝卜酒具有清除肾结石和治疗肝脏和胆囊疾患的作用。红萝卜含有钾和少量铁，不含脂肪，每100克只含有15卡路里的热量。它非常适于制作凉拌沙拉，配上柠檬和盐就是一道佳肴。此外，吃红萝卜还可以控制前列腺癌变。\\n8、番茄含有番茄红素和大量抗氧化剂，能够降低患上慢性疾病的危险，尤其是前列腺癌和心血管疾病。番茄具有提神、助消化和抗炎的作用。用它制作的沙拉、酱汁和菜泥可以帮助患有胃炎和胃溃疡的人更好地消化不易消化吸收的食物。番茄的热量很低，含有维生素C。它富含的番茄红素可以防止罹患前列腺癌。如果使用食用油烹调番茄，还可以增强这种功效。\\n'}], 'tags': ['饮食', '食品', '食疗', '科学', '健康', '食品类型'], 'url': 'http://baike.baidu.com/view/0010.htm'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filepath = 'F:/BaiduNetdiskDownload/data/pretrain_data/563w_baidubaike.json'\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        print(data)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94910bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction from: F:/BaiduNetdiskDownload/data/pretrain_data/563w_baidubaike.json\n",
      "Saving filtered text to: F:/BaiduNetdiskDownload/data/pretrain_data/extracted_baidubaike.jsonl\n",
      "Minimum text length: 30 characters\n",
      "Processed 100000 lines, extracted 255287 texts...\n",
      "Processed 200000 lines, extracted 512387 texts...\n",
      "Processed 300000 lines, extracted 768716 texts...\n",
      "Processed 400000 lines, extracted 1021070 texts...\n",
      "Processed 500000 lines, extracted 1279063 texts...\n",
      "Processed 600000 lines, extracted 1534113 texts...\n",
      "Processed 700000 lines, extracted 1791387 texts...\n",
      "Processed 800000 lines, extracted 2046983 texts...\n",
      "Processed 900000 lines, extracted 2302563 texts...\n",
      "Processed 1000000 lines, extracted 2553785 texts...\n",
      "Processed 1100000 lines, extracted 2807269 texts...\n",
      "Processed 1200000 lines, extracted 3055883 texts...\n",
      "Processed 1300000 lines, extracted 3307424 texts...\n",
      "Processed 1400000 lines, extracted 3558650 texts...\n",
      "Processed 1500000 lines, extracted 3810628 texts...\n",
      "Processed 1600000 lines, extracted 4061782 texts...\n",
      "Processed 1700000 lines, extracted 4314067 texts...\n",
      "Processed 1800000 lines, extracted 4569306 texts...\n",
      "Processed 1900000 lines, extracted 4819049 texts...\n",
      "Processed 2000000 lines, extracted 5067745 texts...\n",
      "Processed 2100000 lines, extracted 5323865 texts...\n",
      "Processed 2200000 lines, extracted 5578881 texts...\n",
      "Processed 2300000 lines, extracted 5825607 texts...\n",
      "Processed 2400000 lines, extracted 6079789 texts...\n",
      "Processed 2500000 lines, extracted 6334442 texts...\n",
      "Processed 2600000 lines, extracted 6588549 texts...\n",
      "Processed 2700000 lines, extracted 6835072 texts...\n",
      "Processed 2800000 lines, extracted 7092180 texts...\n",
      "Processed 2900000 lines, extracted 7348323 texts...\n",
      "Processed 3000000 lines, extracted 7602504 texts...\n",
      "Processed 3100000 lines, extracted 7856990 texts...\n",
      "Processed 3200000 lines, extracted 8112696 texts...\n",
      "Processed 3300000 lines, extracted 8363598 texts...\n",
      "Processed 3400000 lines, extracted 8626098 texts...\n",
      "Processed 3500000 lines, extracted 8882951 texts...\n",
      "Processed 3600000 lines, extracted 9137478 texts...\n",
      "Processed 3700000 lines, extracted 9393523 texts...\n",
      "Processed 3800000 lines, extracted 9644632 texts...\n",
      "Processed 3900000 lines, extracted 9896490 texts...\n",
      "Processed 4000000 lines, extracted 10147552 texts...\n",
      "Processed 4100000 lines, extracted 10398885 texts...\n",
      "Processed 4200000 lines, extracted 10649929 texts...\n",
      "Processed 4300000 lines, extracted 10901556 texts...\n",
      "Processed 4400000 lines, extracted 11154611 texts...\n",
      "Processed 4500000 lines, extracted 11409155 texts...\n",
      "Processed 4600000 lines, extracted 11661792 texts...\n",
      "Processed 4700000 lines, extracted 11907426 texts...\n",
      "Processed 4800000 lines, extracted 12162296 texts...\n",
      "Processed 4900000 lines, extracted 12417822 texts...\n",
      "Processed 5000000 lines, extracted 12671575 texts...\n",
      "Processed 5100000 lines, extracted 12918830 texts...\n",
      "Processed 5200000 lines, extracted 13175306 texts...\n",
      "Processed 5300000 lines, extracted 13432873 texts...\n",
      "Processed 5400000 lines, extracted 13681851 texts...\n",
      "Processed 5500000 lines, extracted 13932908 texts...\n",
      "Processed 5600000 lines, extracted 14191487 texts...\n",
      "\n",
      "Finished processing.\n",
      "Total lines processed: 5634898\n",
      "Total texts extracted and saved: 14276861\n",
      "Output file saved successfully: F:/BaiduNetdiskDownload/data/pretrain_data/extracted_baidubaike.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_and_filter_text(input_filepath, output_filepath, min_length=30):\n",
    "    \"\"\"\n",
    "    Reads a large JSONL file, extracts 'summary' and 'content' from 'sections',\n",
    "    filters out null or short texts, and saves them to a new JSONL file.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Path to the input JSONL file.\n",
    "        output_filepath (str): Path to save the output JSONL file.\n",
    "        min_length (int): Minimum character length for text to be included.\n",
    "    \"\"\"\n",
    "    print(f\"Starting extraction from: {input_filepath}\")\n",
    "    print(f\"Saving filtered text to: {output_filepath}\")\n",
    "    print(f\"Minimum text length: {min_length} characters\")\n",
    "\n",
    "    processed_lines = 0\n",
    "    extracted_count = 0\n",
    "\n",
    "    try:\n",
    "        # Open input and output files simultaneously\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as infile, \\\n",
    "             open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "            for line in infile:\n",
    "                processed_lines += 1\n",
    "                try:\n",
    "                    # Load the JSON data from the current line\n",
    "                    data = json.loads(line)\n",
    "\n",
    "                    texts_to_check = []\n",
    "\n",
    "                    # 1. Check the 'summary' field\n",
    "                    summary = data.get('summary') # Use .get() to avoid KeyError if 'summary' is missing\n",
    "                    if summary: # Check if summary is not None or empty string\n",
    "                        texts_to_check.append(summary)\n",
    "\n",
    "                    # 2. Check the 'content' field within each section\n",
    "                    sections = data.get('sections') # Use .get() for safety\n",
    "                    if isinstance(sections, list): # Ensure 'sections' exists and is a list\n",
    "                        for section in sections:\n",
    "                            content = section.get('content') # Use .get() for safety\n",
    "                            if content: # Check if content is not None or empty string\n",
    "                                texts_to_check.append(content)\n",
    "\n",
    "                    # 3. Filter and write qualifying texts\n",
    "                    for text in texts_to_check:\n",
    "                        # Ensure it's a string and meets the length requirement\n",
    "                        if isinstance(text, str) and len(text.strip()) >= min_length:\n",
    "                             # Create the output dictionary\n",
    "                             output_record = {'text': text.strip()} # Strip leading/trailing whitespace\n",
    "\n",
    "                             # Write the record as a JSON string to the output file, followed by a newline\n",
    "                             outfile.write(json.dumps(output_record, ensure_ascii=False) + '\\n')\n",
    "                             extracted_count += 1\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping malformed JSON line {processed_lines}: {line.strip()[:100]}...\") # Log malformed lines\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: An unexpected error occurred processing line {processed_lines}: {e}\") # Log other errors\n",
    "                    print(f\"Problematic line data (first 100 chars): {line.strip()[:100]}...\")\n",
    "\n",
    "                # Optional: Print progress periodically for large files\n",
    "                if processed_lines % 100000 == 0:\n",
    "                    print(f\"Processed {processed_lines} lines, extracted {extracted_count} texts...\")\n",
    "\n",
    "        print(\"\\nFinished processing.\")\n",
    "        print(f\"Total lines processed: {processed_lines}\")\n",
    "        print(f\"Total texts extracted and saved: {extracted_count}\")\n",
    "        print(f\"Output file saved successfully: {output_filepath}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {input_filepath}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Could not read/write file. Check permissions or disk space. Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "input_filepath = 'F:/BaiduNetdiskDownload/data/pretrain_data/563w_baidubaike.json'\n",
    "output_filepath = 'F:/BaiduNetdiskDownload/data/pretrain_data/extracted_baidubaike.jsonl'\n",
    "min_char_length = 30\n",
    "\n",
    "# --- Run the extraction ---\n",
    "extract_and_filter_text(input_filepath, output_filepath, min_char_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9219f",
   "metadata": {},
   "source": [
    "### 4. 处理sft数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81bf1724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'instruction': '', 'input': '好的。现在请你将这个文本中的所有的逗号都替换成空格。', 'output': '好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？', 'history': [['给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\\n文本：\"这是一个测试句子，目的是看看模型是否可以正确地从这个句子中删除关键词。\"\\\\n关键词列表：[‘测试’，‘模型’]', '删除包含所有给定关键词的子字符串后，文本变为：\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\"']], 'language': 'chinese', 'data_source': 'https://huggingface.co/datasets/BelleGroup/train_3.5M_CN', 'input_len': 59, 'output_len': 66, 'num_utter': 2, 'type': 31, 'type_keyword': ['字符串', '代码', '函数', '编写', '实现', '给定', '使用', '输入', '文本', '程序']} \n",
      "\n",
      "<s>system\n",
      "你是一个AI大语言模型助手，善于解答各种问题。</s>\n",
      "<s>user\n",
      "好的。现在请你将这个文本中的所有的逗号都替换成空格。</s>\n",
      "<s>assistant\n",
      "好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 查看数据集\n",
    "root_path = 'F:/BaiduNetdiskDownload/data/sft_data'\n",
    "file_name = 'sft_data_zh.jsonl'\n",
    "file_path = os.path.join(root_path, file_name)\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        print(data, '\\n')\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"<s>system\\n你是一个AI大语言模型助手，善于解答各种问题。</s>\\n\"},\n",
    "                {\"role\": \"user\", \"content\": f'{data['input']}'},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{data['output']}\"}\n",
    "            ]\n",
    "        input_ids = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        print(input_ids)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908ab708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 11381621/11381621 [02:39<00:00, 71408.02 lines/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据处理完成！共处理并写入 737783 条有效数据到 F:/BaiduNetdiskDownload/data/sft_data/sft_data_zh.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # 导入 tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def chinese_ratio(text: str) -> float:\n",
    "    \"\"\"计算文本中中文字符的比例。\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text)\n",
    "    return len(chinese_chars) / len(text)\n",
    "\n",
    "# 根据是否需要微调历史对话选择contain_history是否为True\n",
    "def process_deepctrl(jsonl_path: str, csv_path: str, contain_history: bool = False, chunk_size: int = 1000):\n",
    "    \"\"\"\n",
    "    处理 SFT 数据，提取输入、输出和可选的历史对话，并写入 CSV\n",
    "\n",
    "    Args:\n",
    "        jsonl_path (str): 输入 JSON Lines 文件路径。\n",
    "        csv_path (str): 输出 CSV 文件路径。\n",
    "        contain_history (bool, optional): 是否包含历史对话。默认为 False。\n",
    "        chunk_size (int, optional): 每次写入 CSV 的数据块大小。默认为 1000。\n",
    "    \"\"\"\n",
    "    # 如果包含历史，修改输出文件名\n",
    "    if contain_history:\n",
    "        dir_name, file_name = os.path.split(csv_path)\n",
    "        name, ext = os.path.splitext(file_name)\n",
    "        new_file_name = name + \"_history\" + ext\n",
    "        csv_path = os.path.join(dir_name, new_file_name)\n",
    "\n",
    "    # 准备写入 CSV\n",
    "    header = ['history', 'q', 'a']\n",
    "    try:\n",
    "        # 尝试获取文件总行数以提供准确的进度条\n",
    "        total_lines = sum(1 for _ in open(jsonl_path, 'rb')) # 使用 'rb' 更快地计数\n",
    "    except Exception:\n",
    "        total_lines = None # 如果文件太大或无法读取，则不显示总数\n",
    "\n",
    "    # 初始化 CSV 文件并写入表头\n",
    "    pd.DataFrame(columns=header).to_csv(\n",
    "        csv_path, index=False, lineterminator='\\n', quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "\n",
    "    processed_count = 0\n",
    "    valid_chunk_data = []\n",
    "\n",
    "    try:\n",
    "        with jsonlines.open(jsonl_path) as reader, \\\n",
    "             tqdm(total=total_lines, desc=\"Processing lines\", unit=\" lines\") as pbar:\n",
    "\n",
    "            for idx, obj in enumerate(reader):\n",
    "                pbar.update(1) # 每次读取一行就更新进度条\n",
    "                try:\n",
    "                    # 1. 提取和合并数据 (确保 q/a 至少有其一或 input/output)\n",
    "                    q = obj.get('input', '') + obj.get('q', '')\n",
    "                    a = obj.get('output', '') + obj.get('a', '')\n",
    "                    history_raw = obj.get('history', []) # 默认空列表更安全\n",
    "\n",
    "                    # 确保 history 是列表，以防万一格式不规范 (如为 null 或字符串)\n",
    "                    if not isinstance(history_raw, list):\n",
    "                        history_raw = []\n",
    "\n",
    "                    # 2. 基本过滤：检查必须字段\n",
    "                    if not q or not a:\n",
    "                        continue\n",
    "                    if contain_history and not history_raw: # 如果需要历史但历史为空，跳过\n",
    "                        continue\n",
    "\n",
    "                    # 3. 长度过滤\n",
    "                    if len(q) < 5 or len(a) < 5:\n",
    "                        continue\n",
    "\n",
    "                    history_len = 0\n",
    "                    if contain_history and history_raw:\n",
    "                        # 确保 history 内部结构是 [q, a] 对\n",
    "                        if all(isinstance(item, list) and len(item) == 2 for item in history_raw):\n",
    "                            history_len = sum(len(h_q) + len(h_a) for h_q, h_a in history_raw)\n",
    "                        else:\n",
    "                            # 如果 history 内部结构不规范，可以选择跳过或记录日志\n",
    "                            # print(f\"Skipping line {idx+1}: Invalid history structure.\")\n",
    "                            continue # 跳过此行\n",
    "                    \n",
    "                     # 总字符长度过滤，同时确保没有过长的q或a，使过滤出的数据是短问题，长回答\n",
    "                    if history_len + len(q) + len(a) > 450 or len(q)>50 or len(a)>400:\n",
    "                        continue\n",
    "\n",
    "                    # 4. 语言过滤 (高中文比例)\n",
    "                    if not (chinese_ratio(q) > 0.9 and chinese_ratio(a) > 0.9):\n",
    "                        continue\n",
    "\n",
    "                    # 5. 构建有效记录\n",
    "                    valid_record = {\n",
    "                        'history': history_raw if contain_history else [],\n",
    "                        'q': q,\n",
    "                        'a': a\n",
    "                    }\n",
    "                    valid_chunk_data.append(valid_record)\n",
    "                    processed_count += 1\n",
    "\n",
    "                    # 6. 分块写入 CSV\n",
    "                    if len(valid_chunk_data) >= chunk_size:\n",
    "                        df_chunk = pd.DataFrame(valid_chunk_data)\n",
    "                        df_chunk.to_csv(\n",
    "                            csv_path, mode='a', header=False, index=False,\n",
    "                            lineterminator='\\n', quoting=csv.QUOTE_MINIMAL\n",
    "                        )\n",
    "                        valid_chunk_data = [] # 清空块\n",
    "\n",
    "                except jsonlines.InvalidLineError:\n",
    "                    print(f\"\\nSkipping invalid JSON line {idx + 1}\")\n",
    "                    continue\n",
    "                except Exception as e: # 捕捉其他潜在错误\n",
    "                    print(f\"\\nError processing line {idx + 1}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # 处理并写入最后一个不足 chunk_size 的块\n",
    "            if valid_chunk_data:\n",
    "                df_chunk = pd.DataFrame(valid_chunk_data)\n",
    "                df_chunk.to_csv(\n",
    "                    csv_path, mode='a', header=False, index=False,\n",
    "                    lineterminator='\\n', quoting=csv.QUOTE_MINIMAL\n",
    "                )\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：输入文件 {jsonl_path} 未找到。\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中发生未预料的错误: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n数据处理完成！共处理并写入 {processed_count} 条有效数据到 {csv_path}\")\n",
    "\n",
    "\n",
    "jsonl_path = \"F:/BaiduNetdiskDownload/data/sft_data/sft_data_zh.jsonl\"\n",
    "csv_path = \"F:/BaiduNetdiskDownload/data/sft_data/sft_data_zh.csv\"\n",
    "process_deepctrl(jsonl_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a79a06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>history</th>\n",
       "      <th>q</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>请从这篇文章中提取出关于分类垃圾的解决方案。</td>\n",
       "      <td>文章中提到的第二个方面——分类垃圾，是解决垃圾污染的有效措施之一。垃圾分类是指将生活垃圾按照...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>根据以下文本，对此事件进行分类：中国队在足球比赛中赢得了冠军。</td>\n",
       "      <td>这个事件可以被分类为体育比赛。具体地，中国队在足球比赛中致胜并赢得了冠军。由此可以推断出，这...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>回答问题并给出详细的推理过程：为什么在地球上的物体会受到重力而不会飘起来？</td>\n",
       "      <td>重力是地球的吸引力。根据牛顿定律，物体会保持以恒定的速度直线运动，直到有另一个力使它改变方向...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>请告诉我什么是机器学习。</td>\n",
       "      <td>机器学习是一种人工智能技术，它使用算法和数学模型，让计算机自动地从数据中学习，并根据这些数据...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>请解释一下什么是人工智能。</td>\n",
       "      <td>人工智能是一种模拟人类智能的技术和方法。它的发展包括机器学习、自然语言处理、计算机视觉等技术...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  history                                      q  \\\n",
       "0      []                 请从这篇文章中提取出关于分类垃圾的解决方案。   \n",
       "1      []        根据以下文本，对此事件进行分类：中国队在足球比赛中赢得了冠军。   \n",
       "2      []  回答问题并给出详细的推理过程：为什么在地球上的物体会受到重力而不会飘起来？   \n",
       "3      []                           请告诉我什么是机器学习。   \n",
       "4      []                          请解释一下什么是人工智能。   \n",
       "\n",
       "                                                   a  \n",
       "0  文章中提到的第二个方面——分类垃圾，是解决垃圾污染的有效措施之一。垃圾分类是指将生活垃圾按照...  \n",
       "1  这个事件可以被分类为体育比赛。具体地，中国队在足球比赛中致胜并赢得了冠军。由此可以推断出，这...  \n",
       "2  重力是地球的吸引力。根据牛顿定律，物体会保持以恒定的速度直线运动，直到有另一个力使它改变方向...  \n",
       "3  机器学习是一种人工智能技术，它使用算法和数学模型，让计算机自动地从数据中学习，并根据这些数据...  \n",
       "4  人工智能是一种模拟人类智能的技术和方法。它的发展包括机器学习、自然语言处理、计算机视觉等技术...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9676f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据以下文本，对此事件进行分类：中国队在足球比赛中赢得了冠军。\n",
      "这个事件可以被分类为体育比赛。具体地，中国队在足球比赛中致胜并赢得了冠军。由此可以推断出，这场比赛可能是由各国派出的足球队伍参加的国际足球比赛，中国队在比赛中表现出色并赢得了锦标。 这个事件的重要性对于中国足球运动员和球迷以及全国范围的足球运动发展都具有重要意义。\n",
      "910853\n",
      "<s>system\n",
      "你是一个AI大语言模型助手，善于解答各种问题。</s>\n",
      "<s>user\n",
      "根据以下文本，对此事件进行分类：中国队在足球比赛中赢得了冠军。</s>\n",
      "<s>assistant\n",
      "这个事件可以被分类为体育比赛。具体地，中国队在足球比赛中致胜并赢得了冠军。由此可以推断出，这场比赛可能是由各国派出的足球队伍参加的国际足球比赛，中国队在比赛中表现出色并赢得了锦标。 这个事件的重要性对于中国足球运动员和球迷以及全国范围的足球运动发展都具有重要意义。</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "sample = df.iloc[index]\n",
    "question = sample['q']\n",
    "answer = sample['a']\n",
    "print(question)\n",
    "print(answer)\n",
    "print(len(df))\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"<s>system\\n你是一个AI大语言模型助手，善于解答各种问题。</s>\\n\"},\n",
    "    {\"role\": \"user\", \"content\": f'{question}'},\n",
    "    {\"role\": \"assistant\", \"content\": f\"{answer}\"}\n",
    "    ]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9ba10",
   "metadata": {},
   "source": [
    "# 三、查看模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647ce47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"betas\": [\n",
      "        0.9,\n",
      "        0.95\n",
      "    ],\n",
      "    \"bias_update_speed\": 0.001,\n",
      "    \"dim\": 768,\n",
      "    \"epochs\": 1,\n",
      "    \"inter_dim\": 3072,\n",
      "    \"kv_lora_rank\": 256,\n",
      "    \"lr_decay_iters\": 461312,\n",
      "    \"lr_decay_ratio\": 0.98,\n",
      "    \"max_batch_size\": 18,\n",
      "    \"max_lr\": 0.0006,\n",
      "    \"max_new_tokens\": 512,\n",
      "    \"max_seq_len\": 512,\n",
      "    \"min_lr\": 2e-05,\n",
      "    \"moe_inter_dim\": 512,\n",
      "    \"mtp_loss_lambda\": 0.0001,\n",
      "    \"n_activated_experts\": 2,\n",
      "    \"n_dense_layers\": 3,\n",
      "    \"n_expert_groups\": 4,\n",
      "    \"n_heads\": 12,\n",
      "    \"n_layers\": 12,\n",
      "    \"n_limited_groups\": 2,\n",
      "    \"n_routed_experts\": 8,\n",
      "    \"n_shared_experts\": 1,\n",
      "    \"q_lora_rank\": 384,\n",
      "    \"qk_nope_head_dim\": 64,\n",
      "    \"qk_rope_head_dim\": 32,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"route_scale\": 1.0,\n",
      "    \"seq_aux_alpha\": 0.0001,\n",
      "    \"temperature\": 1.0,\n",
      "    \"top_k\": null,\n",
      "    \"top_p\": null,\n",
      "    \"use_mtp\": true,\n",
      "    \"use_noaux_tc\": true,\n",
      "    \"use_seq_aux\": true,\n",
      "    \"v_head_dim\": 64,\n",
      "    \"vocab_size\": 32000,\n",
      "    \"warmup_iters\": 23536,\n",
      "    \"warmup_ratio\": 0.05,\n",
      "    \"weight_decay\": 0.01\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from utils.little_tools import load_yaml\n",
    "import json\n",
    "\n",
    "root_path = \"C:/Users/WKQ/Downloads/pretrained_model/\"\n",
    "model_name = \"mini_deepseekv3\"\n",
    "yaml_path = root_path + model_name + \"/pretrained_mini_deepseekv3_model_args.yaml\"\n",
    "\n",
    "args = load_yaml(yaml_path)\n",
    "\n",
    "print(json.dumps(args, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
