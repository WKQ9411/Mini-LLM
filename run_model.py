import streamlit as st
from model import get_model_and_args # å‡è®¾ model.py å’Œè¿™ä¸ªå‡½æ•°å­˜åœ¨ä¸”å¯ç”¨
import torch
from transformers import AutoTokenizer
import os # Import os for path joining if needed, although not used for the fixed path

# --- Configuration ---
# Define constants in the global scope
TOKENIZER_NAME = "./mini_tokenizer"
MODEL_NAME = "mini_llama3"  # <<< DEFINE MODEL_NAME HERE
MODEL_PATH = "C:\\Users\\WKQ\\Downloads\\pretrained_mini_llama3_epoch_1_iter_70000_loss_3.209038257598877-base.pt" # ä¿æŒç”¨æˆ·æŒ‡å®šè·¯å¾„

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Mini DeepSeek èŠå¤©åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ¨¡å‹åˆå§‹åŒ–å‡½æ•° (åªè¿è¡Œä¸€æ¬¡)
@st.cache_resource
def load_model():
    """Loads the model and tokenizer."""
    # Use the globally defined constants
    tokenizer_name = TOKENIZER_NAME
    model_name = MODEL_NAME # Use global MODEL_NAME
    model_path = MODEL_PATH   # Use global MODEL_PATH

    # åŠ è½½tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    except Exception as e:
        st.error(f"åŠ è½½ Tokenizer å‡ºé”™ ({tokenizer_name}): {e}")
        st.stop()
        return None, None, None # Match expected return values on error

    # åˆå§‹åŒ–æ¨¡å‹
    try:
        Model, Model_Args = get_model_and_args(model_name) # Use model_name here
        model_args = Model_Args(max_batch_size=1, max_seq_len=256)
        model = Model(model_args)
        model.eval()
    except NameError:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹å‡ºé”™: 'get_model_and_args' æœªåœ¨ 'model.py' ä¸­æ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚")
        st.stop()
        return None, None, None
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹å‡ºé”™ (æ¨¡å‹å: {model_name}): {e}") # model_name is accessible here
        st.error("è¯·ç¡®è®¤ 'model.py' æ–‡ä»¶å­˜åœ¨ä¸”åŒ…å«æ­£ç¡®çš„ 'get_model_and_args' å‡½æ•°åŠæ¨¡å‹ç±»ã€‚")
        st.stop()
        return None, None, None

    # åŠ è½½æ¨¡å‹æƒé‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    try:
        # It's often better to move model to device *before* loading state dict
        model.to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        # model.to(device) # Can be here or before load_state_dict
    except FileNotFoundError:
        st.error(f"æ¨¡å‹æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
        st.error("è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ä¸”æ–‡ä»¶å­˜åœ¨ã€‚")
        st.stop()
        return None, None, None
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹æƒé‡æˆ–ç§»åŠ¨åˆ°è®¾å¤‡å‡ºé”™ ({model_path}): {e}")
        st.stop()
        return None, None, None

    return model, tokenizer, device # Return device

# åŠ è½½æ¨¡å‹
try:
    with st.spinner('æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...'):
        # The function now only returns 3 items
        model, tokenizer, device = load_model()
        if model is None or tokenizer is None:
             st.error("æ¨¡å‹æˆ–TokenizeråŠ è½½å¤±è´¥ï¼Œåº”ç”¨æ— æ³•å¯åŠ¨ã€‚")
             st.stop() # å¦‚æœåŠ è½½å¤±è´¥åˆ™åœæ­¢
    st.success(f"æ¨¡å‹åŠ è½½æˆåŠŸ! (è¿è¡Œäº {device.upper()})") # æ˜¾ç¤ºè¿è¡Œè®¾å¤‡
except Exception as e:
     st.error(f"åŠ è½½æ¨¡å‹è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æ–™çš„é”™è¯¯: {e}")
     st.stop()


# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.title("æ¨¡å‹è®¾ç½®")

    # å‚æ•°è°ƒèŠ‚
    temperature = st.slider("Temperature", min_value=0.0, max_value=2.0, value=1.0, step=0.05,
                           help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚è¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼æ›´éšæœºã€‚(å»ºè®® 0.7-1.0)")
    top_k = st.slider("Top-k", min_value=1, max_value=100, value=50, step=1,
                     help="é™åˆ¶ç”Ÿæˆæ—¶åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯ã€‚(å»ºè®® 40-60)")
    top_p = st.slider("Top-p (Nucleus Sampling)", min_value=0.0, max_value=1.0, value=0.9, step=0.01,
                     help="é™åˆ¶ç”Ÿæˆæ—¶åªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pçš„æœ€å°è¯é›†ã€‚ä¸Top-käºŒé€‰ä¸€æˆ–ç»“åˆä½¿ç”¨ã€‚(å»ºè®® 0.85-0.95)")
    max_length = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", min_value=50, max_value=512, value=200, step=10,
                          help="é™åˆ¶å•æ¬¡å›å¤ç”Ÿæˆçš„æœ€å¤§Tokenæ•°é‡")

    # æ¨¡å‹ä¿¡æ¯
    st.divider()
    st.subheader("æ¨¡å‹ä¿¡æ¯")
    approx_params_display = "æœªçŸ¥"
    try:
        _, approx_params = model.count_parameters()
        approx_params_display = f"{approx_params:,}" if isinstance(approx_params, int) else str(approx_params)
    except AttributeError:
        approx_params_display = "æ— æ³•è·å– (æ–¹æ³•ä¸å­˜åœ¨)"
    except Exception as e:
        approx_params_display = f"è·å–é”™è¯¯: {e}"

    st.write(f"æ¨¡å‹å‚æ•°é‡: {approx_params_display}")
    max_context = 512 # Keep hardcoded or retrieve if possible
    st.write(f"æ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {max_context}")
    st.write(f"è¿è¡Œè®¾å¤‡: {device.upper()}")

    st.divider()
    if st.button("æ¸…é™¤èŠå¤©è®°å½•"):
        st.session_state.messages = []
        st.rerun()

    st.caption("Mini DeepSeek èŠå¤©åŠ©æ‰‹ v1.0")

# ä¸»ç•Œé¢
st.title("ğŸ¤– Mini DeepSeek èŠå¤©åŠ©æ‰‹")
# Now MODEL_NAME is accessible here because it's defined globally
st.caption(f"ä¸€ä¸ªåŸºäº {MODEL_NAME} æ¨¡å‹çš„å¯¹è¯åŠ©æ‰‹")

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        model_input_prompt = prompt # Still using only current prompt

        try:
            with st.spinner("æ€è€ƒä¸­..."):
                output_generator = model.generate(
                    prompt=model_input_prompt,
                    context_length=max_length,
                    tokenizer=tokenizer,
                    stream=True,
                    task="generate",
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                )

                for output_chunk in output_generator:
                    if isinstance(output_chunk, str):
                        full_response += output_chunk
                        message_placeholder.markdown(full_response + "â–Œ")

            message_placeholder.markdown(full_response)

        except AttributeError as e:
             st.error(f"æ¨¡å‹ç”Ÿæˆå‡ºé”™: 'generate' æ–¹æ³•å¯èƒ½ä¸å­˜åœ¨æˆ–å‚æ•°ä¸åŒ¹é…ã€‚é”™è¯¯: {e}")
             st.error("è¯·æ£€æŸ¥ model.py ä¸­çš„ 'generate' å‡½æ•°å®šä¹‰åŠå…¶æ¥å—çš„å‚æ•°ï¼ˆæ˜¯å¦åŒ…å« temperature, top_k, top_p, max_length ç­‰ï¼‰ã€‚")
             full_response = "æŠ±æ­‰ï¼Œæ¨¡å‹æ¥å£è°ƒç”¨å‡ºé”™ã€‚"
             message_placeholder.markdown(full_response)
        except Exception as e:
             st.error(f"ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
             full_response = "æŠ±æ­‰ï¼Œæˆ‘åœ¨å°è¯•ç”Ÿæˆå›å¤æ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
             message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})